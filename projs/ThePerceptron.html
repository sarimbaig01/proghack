<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Perceptron</title>
    <link rel="stylesheet" href="../prism.css"> <!-- Adjust the path as needed -->
    <link rel="stylesheet" href="../style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
   <a href="https://sarimbaig01.github.io/proghack/" class="back-link top-right">← Back</a>

    <div class="toc">
    <section class="toc">
        <h2>Contents</h2>
        <a href="#intro">Introduction</a>
        <a href="#terms1">Basic terminology</a>
        <a href="#gates">The Perceptron as a logic gate</a>
        <a href="#terms2">Further terminology</a>
        <a href="#task1">Task 1: Code the Perceptron algorithm</a>
       
        <!--
        <a href="#dir" class="sub-item">Add direction</a>
        <a href="#edge" class="sub-item">Detect edges and change direction</a>
        <a href="#colors" class="sub-item">Color-code for age</a>
        <a href="#equi" class="sub-item">Detect equilibrium</a>
        <a href="#confs">Simulate different initial configurations</a>
        -->
    </section>
    </div>

    <div class="statement">
        
        <h1>The Perceptron</h1>
        
    <section class="statement">

       <div class="intro" id="intro">
            <h2>Introduction</h2>
            <p>A perceptron is one of the simplest forms of artificial neural networks, designed to perform binary classification tasks. Inspired by the way biological neurons function, a perceptron takes multiple input signals, applies corresponding weights, sums them up, and passes the result through an activation function to produce an output. This output determines which class the input belongs to. A single-layer perceptron is the most basic form, but by stacking multiple layers of perceptrons, we can create multi-layer perceptrons (MLPs) or more complex neural networks capable of solving a wide range of problems. The perceptron is used in supervised learning, where it learns from labeled data to make predictions. It forms the foundation for more complex neural network architectures and serves as a crucial building block in the field of machine learning.</p>
            <p>Examples of supervised learning tasks that could use a perceptron include medical diagnosis, where symptoms are used to determine whether a patient has a particular disease, and sentiment analysis, where text is classified as having positive or negative sentiment, etc.</p>
        </div>

        <div class="terms1" id="terms1">
            <fieldset>
                <legend>Basic terminology</legend>
                
                <p><strong>Inputs (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>m</sub>)</strong>: The features or data points fed into a model for processing and prediction. Each input is typically a vector of values representing different attributes or characteristics of the data.</p>
                
                <p><strong>Weights (w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>m</sub>)</strong>: Values that are multiplied by input features. They determine how much influence each input has on the output.</p>
                
                <p><strong>Bias (w<sub>0</sub>)</strong>: An extra value added to the weighted sum of inputs (it could be thought of as the weight associated with another input which is always 1). It helps the neuron make better decisions by shifting the activation function.</p>
                
                <p><strong>Weighted Sum</strong>: The sum of the products of the weights and inputs plus the bias before being passed to the activation function. It is the intermediate value computed by the neuron.<br>
                \( z = w_1 x_1 + w_2 x_2 + \cdots + w_m x_m + w_0 \)
                </p>
                
                <p><strong>Activation Function</strong>: A function applied to the weighted sum in a neuron to produce an output. It can be a step function or other non-linear functions like sigmoid or ReLU.<br>
                \( \hat{y} = \text{step}(z) \)<br>
                where the step function is defined as:<br>
                \[
                \text{step}(z) = 
                \begin{cases} 
                1 & \text{if } z \geq 0 \\
                0 & \text{if } z < 0 
                \end{cases}
                \]
                See also Figure 1. 
                </p>

                <p><strong>Predicted Output (\(\hat{y}\)):</strong> The output generated by the perceptron after applying the activation function to the weighted sum.</p>

                <p><strong>Actual Output (\(y\)):</strong> The true output value from the dataset used for training.</p>

                <p><strong>Training (data) Sample (\(x_1, x_2, ..., x_m, y\)):</strong> A single instance from the dataset, consisting of input features (\(x_1, x_2, ..., x_n\)) and the actual output (\(y\)).</p>

                <figure>
                    <img src="./img/ThePerceptron1.jpg" width="500">
                    <figcaption>Figure 1: A visual representation of a Perceptron with inputs, weights, weighted sum, and a step activation function. (Image source: Shanmugamani, R. (2018) Deep Learning for Computer Vision, Birmingham, UK: Packt Publishing)</figcaption>
                </figure>
            </fieldset>
        </div>

        <div class="gates" id="gates">
            <h2>Example: The Perceptron as a logic gate</h2>
            <p>The perceptron can perfectly classify the inputs \( x_1 \) and \( x_2 \) for AND, OR, and NOT gates because these logic gates are linearly separable. This means the inputs and outputs can be divided with a straight line, allowing the perceptron to create a clear decision boundary using a step activation function.</p>
            <p>For an AND gate, the perceptron can be set up with weights \( w_1 = 1 \) and \( w_2 = 1 \), and a bias \( w_0 = -1.5 \). The weighted sum \( z = w_1 x_1 + w_2 x_2 + w_0 \) becomes \( z = x_1 + x_2 - 1.5 \). The perceptron will output 1 (true) only when both \( x_1 \) and \( x_2 \) are 1, making the weighted sum greater than or equal to 0.</p>
            <p>Similarly, appropriate values for weights and biases can be found for OR, NOT, NAND, and NOR gates to achieve perfect classification.</p>
            <p>These weights can be manually set for simple logic gates, but for larger datasets and more complex tasks, the weights are typically learned through a training process. This process involves adjusting the weights based on the error between the predicted output and the actual output, iteratively minimizing this error.</p>
            <p>Figure 2 below shows the situation with the logic gates on a 2D plane. In particular, note that the outputs of a XOR gate are not perfectly linearly separable.</p>
            <figure>
                <img src="./img/ThePerceptron2.jpg">
                <figcaption>Figure 2: Outputs of AND, OR and XOR gates shown on a 2D plane. Whereas the outputs of the AND and OR gates are linearly separable, outputs of the XOR gate are not. Similarly, for more complex datasets we will encounter some classification error and the aim would be to set the weights in such a way so that the error is minimized. (Image source: https://gamedevacademy.org/perceptrons-the-first-neural-networks/)</figcaption>
            </figure>
        </div>

        <div class="terms2" id="terms2">
            <fieldset>
                <legend>Further terminology</legend>
                
                <p><strong>Error</strong>: The difference between the predicted output and the actual output. It is used to adjust the model’s weights during training.</p>
                
                <p><strong>Learning Rate (&eta)</strong>: A small value that controls how much the weights are updated during training. It affects how quickly the model learns.</p>
                
                <p><strong>Training Data</strong>: The set of data used to teach the model. It includes input-output pairs where the correct output is known.</p>
                
                <p><strong>Test Data</strong>: A final set of data used to evaluate the model’s performance after training. It ensures the model works well on new, unseen data.</p>
                
            </fieldset>
        </div>

        <div class="task1" id="task1">
            <h2>Task 1: Code the Perceptron algorithm</h2>
            <p>Figure 3 below shows the training algorithm of the Perceptron. TODO: add here...</p>
            <figure class="no-center">
                <table class="atable">
                    <tr><td>
                        <div class="algorithm">   
                            <p>Initialize weights and bias</p>
                            <p>Repeat for a specified number of training cycles:</p>
                            <p class="indent-1">For each training sample (x1, x2, ..., xm, y):</p>
                            <p class="indent-2">Compute weighted sum \( z \)</p>
                            <p class="indent-2">Predict output \( \hat{y} \)</p>
                            <p class="indent-2">Compute error \( y - \hat{y} \)</p>
                            <p class="indent-2">Update weights and bias according to error and learning rate</p>
                            <p class="indent-1">Print training progress</p>
                            <p>Return final weights and bias</p>
                        </div>
                    </td></tr>
                </table>
                <figcaption>Figure 3: The Perceptron taining algorithm. The training happens for a certain number of training cycles (also called epochs). The inputs to the algorithms are the training data, validation data, and the initial weights of the Perceptron.</figcaption>
            </figure>
        </div>



        

        <!-- Prism.js JavaScript -->
        <script src="../prism.js"></script> <!-- Adjust the path as needed -->
        <script src="../insertCode.js"></script> <!-- Link to your external JavaScript file -->
    
    </section>
    </div>
      
</body>
</html>

